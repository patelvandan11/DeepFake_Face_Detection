{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad4cabee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# from sklearn.cluster import KMeans\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# from sklearn.manifold import TSNE\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\__init__.py:997\u001b[0m\n\u001b[0;32m    992\u001b[0m     _log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded rc file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, fname)\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m--> 997\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Strip leading comment.\u001b[39;49;00m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfail_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m rcParamsDefault\u001b[38;5;241m.\u001b[39m_update_raw(rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m   1003\u001b[0m rcParamsDefault\u001b[38;5;241m.\u001b[39m_ensure_has_backend()\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\__init__.py:934\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m rcsetup\u001b[38;5;241m.\u001b[39m_validators:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fail_on_error:\n\u001b[1;32m--> 934\u001b[0m         \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m val  \u001b[38;5;66;03m# try to convert to proper type or raise\u001b[39;00m\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\__init__.py:769\u001b[0m, in \u001b[0;36mRcParams.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\rcsetup.py:342\u001b[0m, in \u001b[0;36mvalidate_color\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_color_like(stmp):\n\u001b[0;32m    340\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m stmp\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_color_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# If it is still valid, it must be a tuple (as a string from matplotlibrc).\u001b[39;00m\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\colors.py:230\u001b[0m, in \u001b[0;36mis_color_like\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mto_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\colors.py:317\u001b[0m, in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    315\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rgba \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Suppress exception chaining of cache lookup failure.\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m \u001b[43m_to_rgba_no_colorcycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         _colors_full_map\u001b[38;5;241m.\u001b[39mcache[c, alpha] \u001b[38;5;241m=\u001b[39m rgba\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\matplotlib\\colors.py:339\u001b[0m, in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be between 0 and 1, inclusive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    338\u001b[0m orig_c \u001b[38;5;241m=\u001b[39m c\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241m.\u001b[39mmasked:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\numpy\\__init__.py:343\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sanity_check\u001b[39m():\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    Quick sanity checks for common bugs caused by environment.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    There are some cases e.g. with wrong BLAS ABI that cause wrong\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    results under specific runtime conditions that are not necessarily\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    achieved during test suite runs, and it is useful to catch those early.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    See https://github.com/numpy/numpy/issues/8577 and other\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;124;03m    similar bug reports.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m         x \u001b[38;5;241m=\u001b[39m ones(\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mfloat32)\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\numpy\\ma\\__init__.py:42\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mMasked Arrays\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extras\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\numpy\\ma\\core.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mumath\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumerictypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mntypes\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multiarray \u001b[38;5;28;01mas\u001b[39;00m mu\n",
      "File \u001b[1;32md:\\ana\\envs\\gpu-env\\lib\\site-packages\\numpy\\core\\umath.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCreate the numpy.core.umath namespace for backward compatibility. In v1.16\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mthe multiarray and umath c-extension modules were merged into a single\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# These imports are needed for backward compatibility,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# do not change them. issue gh-11862\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# _ones_like is semi-public, on purpose not added to __all__\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _UFUNC_API, _add_newdoc_ufunc, _ones_like\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import models\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b6f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7b3454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------- STEP 0: GPU SETUP --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef37cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_video(video_path, mtcnn, max_frames=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    faces = []\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face = mtcnn(frame_rgb)\n",
    "        if face is not None:\n",
    "            faces.append(face)\n",
    "            frame_count += 1\n",
    "    cap.release()\n",
    "    if len(faces) == max_frames:\n",
    "        return torch.stack(faces)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "class VideoFaceDataset(Dataset):\n",
    "    def __init__(self, video_paths, mtcnn, transform=None, max_frames=30):\n",
    "        self.video_paths = video_paths\n",
    "        self.mtcnn = mtcnn\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        faces = extract_faces_from_video(video_path, self.mtcnn, self.max_frames)\n",
    "        if faces is None:\n",
    "            raise ValueError(f\"Insufficient faces in video: {video_path}\")\n",
    "        if self.transform:\n",
    "            faces = torch.stack([self.transform(face) for face in faces])\n",
    "        return faces, video_path\n",
    "\n",
    "def get_video_paths(main_folder):\n",
    "    paths = []\n",
    "    if not os.path.exists(main_folder):\n",
    "        raise ValueError(f\"Folder not found: {main_folder}\")\n",
    "    for file in os.listdir(main_folder):\n",
    "        if file.endswith(('.mp4', '.avi')):\n",
    "            paths.append(os.path.join(main_folder, file))\n",
    "    return paths\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, feature_size=512, hidden_size=256, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.gru = nn.GRU(input_size=feature_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.resnet(x)\n",
    "        feats = feats.view(B, T, -1)\n",
    "        _, hidden = self.gru(feats)\n",
    "        out = self.classifier(hidden[-1])\n",
    "        return out\n",
    "\n",
    "def extract_video_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_feats = []\n",
    "    video_paths = []\n",
    "    with torch.no_grad():\n",
    "        for faces, paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "            if faces is None or len(faces) == 0:\n",
    "                print(f\"Skipping video (no faces): {paths}\")\n",
    "                continue\n",
    "            faces = faces.to(device)\n",
    "            B, T, C, H, W = faces.size()\n",
    "            faces = faces.view(B * T, C, H, W)\n",
    "            feats = model.resnet(faces)\n",
    "            feats = feats.view(B, T, -1)\n",
    "            avg_feat = feats.mean(dim=1).squeeze().cpu().numpy()\n",
    "            if avg_feat.ndim == 1 and avg_feat.size == 512:\n",
    "                all_feats.append(avg_feat)\n",
    "                video_paths.extend(paths)\n",
    "            else:\n",
    "                print(f\"Skipping video due to invalid feature shape: {paths}\")\n",
    "    if len(all_feats) == 0:\n",
    "        raise ValueError(\"No valid features extracted. Cannot perform clustering.\")\n",
    "    return np.array(all_feats), video_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_dir = \"E:\\DFDC dataset\\dfdc_train_part_00\\dfdc_train_part_0\"  # Change accordingly\n",
    "\n",
    "    print(\"Loading video paths...\")\n",
    "    video_paths = get_video_paths(video_dir)\n",
    "    print(f\"Total videos found: {len(video_paths)}\")\n",
    "\n",
    "    if len(video_paths) == 0:\n",
    "        raise ValueError(\"No videos found in the folder!\")\n",
    "\n",
    "    mtcnn = MTCNN(image_size=160, device=device)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    dataset = VideoFaceDataset(video_paths, mtcnn, transform=transform, max_frames=30)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = GRUClassifier()\n",
    "    model.to(device)\n",
    "\n",
    "    # Extract features\n",
    "    features, paths = extract_video_features(dataloader, model, device)\n",
    "\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "    # Use 1 cluster since only one folder of videos\n",
    "    n_clusters = 1\n",
    "    print(f\"Clustering with {n_clusters} cluster(s)...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features)\n",
    "\n",
    "    # Visualize clusters (t-SNE is still possible even for 1 cluster)\n",
    "    print(\"Visualizing with t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    reduced = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='coolwarm', s=15)\n",
    "    plt.title(\"t-SNE Visualization of Video Feature Clusters\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73c442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825e3cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m7\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plt.scatter(\u001b[43mreduced\u001b[49m[:, \u001b[32m0\u001b[39m], reduced[:, \u001b[32m1\u001b[39m], c=clusters, cmap=\u001b[33m'\u001b[39m\u001b[33mcoolwarm\u001b[39m\u001b[33m'\u001b[39m, s=\u001b[32m15\u001b[39m)\n\u001b[32m      3\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mt-SNE Visualization of Video Feature Clusters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mComponent 1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'reduced' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='coolwarm', s=15)\n",
    "plt.title(\"t-SNE Visualization of Video Feature Clusters\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91fef589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gru_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"main_model_11.pth\")\n",
    "print(\"Model saved as gru_classifier.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fe67909",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------- DEVICE --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------- STEP 1: FACE EXTRACTION --------------------\n",
    "def extract_faces_from_video(video_path, mtcnn, max_frames=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    faces = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face = mtcnn(frame_rgb)\n",
    "        if face is not None:\n",
    "            faces.append(face)\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(faces) == max_frames:\n",
    "        return torch.stack(faces)  # Shape: [30, 3, 160, 160]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_dataset(main_folder, max_frames=30):\n",
    "    mtcnn = MTCNN(image_size=160, device=device)\n",
    "    videos = []\n",
    "    labels = []\n",
    "\n",
    "    for label_name in ['real', 'fake']:\n",
    "        label_folder = os.path.join(main_folder, label_name)\n",
    "        if not os.path.exists(label_folder):\n",
    "            continue\n",
    "        label = 1 if label_name == 'real' else 0\n",
    "\n",
    "        for file in tqdm(os.listdir(label_folder)):\n",
    "            if not file.endswith(('.mp4', '.avi')):\n",
    "                continue\n",
    "            video_path = os.path.join(label_folder, file)\n",
    "            print(f\"Processing: {file} ({label_name})\")\n",
    "            face_seq = extract_faces_from_video(video_path, mtcnn, max_frames)\n",
    "            if face_seq is not None:\n",
    "                videos.append(face_seq)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipped (not enough faces): {file}\")\n",
    "\n",
    "    print(f\"Total valid sequences: {len(videos)}\")\n",
    "    return videos, labels\n",
    "\n",
    "# -------------------- STEP 2: DATASET --------------------\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, face_sequences, labels, transform=None):\n",
    "        self.sequences = face_sequences\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        if self.transform:\n",
    "            seq = torch.stack([self.transform(img) for img in seq])\n",
    "        return seq, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# -------------------- STEP 3: MODEL --------------------\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, feature_size=512, hidden_size=256, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.gru = nn.GRU(input_size=feature_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.resnet(x)\n",
    "        feats = feats.view(B, T, -1)\n",
    "        _, hidden = self.gru(feats)\n",
    "        out = self.classifier(hidden[-1])\n",
    "        return out\n",
    "\n",
    "# -------------------- STEP 4: FEATURE EXTRACTION + CLUSTERING --------------------\n",
    "def extract_video_features(sequences, model):\n",
    "    model.eval()\n",
    "    all_feats = []\n",
    "    with torch.no_grad():\n",
    "        for seq in tqdm(sequences, desc=\"Extracting Features\"):\n",
    "            seq = torch.stack([transforms.Resize((224, 224))(img) for img in seq])\n",
    "            seq = transforms.Normalize([0.5]*3, [0.5]*3)(seq)\n",
    "            seq = seq.unsqueeze(0).to(device)  # [1, T, 3, H, W]\n",
    "\n",
    "            B, T, C, H, W = seq.size()\n",
    "            seq = seq.view(B * T, C, H, W)\n",
    "            feats = model.resnet(seq)  # [B*T, 512]\n",
    "            feats = feats.view(B, T, -1)\n",
    "            avg_feat = feats.mean(dim=1).squeeze().cpu().numpy()  # [512]\n",
    "            all_feats.append(avg_feat)\n",
    "    return all_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- STEP 5: RUN PIPELINE --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    video_dir = \"E:\\DFDC dataset\\dfdc_train_part_00\\dfdc_train_part_0\"  # path to folder containing 'real/' and 'fake/'\n",
    "    face_sequences, labels = load_dataset(video_dir, max_frames=30)\n",
    "\n",
    "    if len(face_sequences) == 0:\n",
    "        raise ValueError(\"No valid face sequences extracted.\")\n",
    "\n",
    "    model = GRUClassifier()\n",
    "    model.to(device)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_video_features(face_sequences, model)\n",
    "\n",
    "    # K-Means Clustering\n",
    "    print(\"Clustering...\")\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features)\n",
    "\n",
    "    # Visualize\n",
    "    print(\"Visualizing...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "    reduced = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='coolwarm')\n",
    "    plt.title(\"t-SNE Clustering of Deepfake Features\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save cluster pseudo-labels\n",
    "    df = pd.DataFrame({\n",
    "        'video_index': list(range(len(clusters))),\n",
    "        'pseudo_label': clusters\n",
    "    })\n",
    "    df.to_csv(\"pseudo_labels.csv\", index=False)\n",
    "    print(\"Cluster assignments saved to 'pseudo_labels.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Save model\n",
    "torch.save(model.state_dict(), \"unsupervised_gru_10.pth\")\n",
    "print(\"Model weights saved to 'unsupervised_gru_10.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------- STEP 1: FACE EXTRACTION --------------------\n",
    "def extract_faces_from_video(video_path, mtcnn, max_frames=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    faces = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face = mtcnn(frame_rgb)\n",
    "        if face is not None:\n",
    "            faces.append(face)\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(faces) == max_frames:\n",
    "        return torch.stack(faces)  # Shape: [30, 3, 160, 160]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_dataset(main_folder, max_frames=30):\n",
    "    mtcnn = MTCNN(image_size=160, device=device)\n",
    "    videos = []\n",
    "    labels = []\n",
    "\n",
    "    for label_name in ['real', 'fake']:\n",
    "        label_folder = os.path.join(main_folder, label_name)\n",
    "        if not os.path.exists(label_folder):\n",
    "            continue\n",
    "        label = 1 if label_name == 'real' else 0\n",
    "\n",
    "        for file in tqdm(os.listdir(label_folder)):\n",
    "            if not file.endswith((\".mp4\", \".avi\")):\n",
    "                continue\n",
    "            video_path = os.path.join(label_folder, file)\n",
    "            print(f\"Processing: {file} ({label_name})\")\n",
    "            face_seq = extract_faces_from_video(video_path, mtcnn, max_frames)\n",
    "            if face_seq is not None:\n",
    "                videos.append(face_seq)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipped (not enough faces): {file}\")\n",
    "\n",
    "    print(f\"Total valid sequences: {len(videos)}\")\n",
    "    return videos, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9615a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- STEP 2: DATASET --------------------\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, face_sequences, labels, transform=None):\n",
    "        self.sequences = face_sequences\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        if self.transform:\n",
    "            seq = torch.stack([self.transform(img) for img in seq])\n",
    "        return seq, torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfba63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- STEP 3: MODEL --------------------\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, feature_size=512, hidden_size=256, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove classification layer\n",
    "\n",
    "        self.gru = nn.GRU(input_size=feature_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, 3, H, W]\n",
    "        B, T, C, H, W = x.size()\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.resnet(x)  # [B*T, 512]\n",
    "        feats = feats.view(B, T, -1)  # [B, T, 512]\n",
    "        _, hidden = self.gru(feats)\n",
    "        out = self.classifier(hidden[-1])  # last hidden state\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- STEP 4: TRAINING --------------------\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        acc = correct / total * 100\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Train Accuracy: {acc:.2f}%\")\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_acc = correct / total * 100\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- STEP 5: RUN FULL PIPELINE --------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_dir = \"E:\\DFDC dataset\\dfdc_train_part_00\\dfdc_train_part_0\"  # Folder with 'real/' and 'fake/' subfolders\n",
    "    face_sequences, labels = load_dataset(video_dir, max_frames=30)\n",
    "\n",
    "    if len(face_sequences) == 0:\n",
    "        raise ValueError(\"No valid face sequences extracted. Check your video files.\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(face_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = DeepfakeDataset(X_train, y_train, transform=transform)\n",
    "    val_dataset = DeepfakeDataset(X_val, y_val, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    model = GRUClassifier()\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff00df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
